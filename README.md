# ğŸ¤– RAG Assistant  
A simple **Retrieval-Augmented Generation (RAG)** assistant built using **Python**, **LangChain**, **DuckDuckGo Search**, and **Ollama** (running Llama 3 locally).  
This project answers real-time questions using **search-based context** and a **local LLM**.

---

## âœ¨ Features

- ğŸ” Uses **DuckDuckGo Search** to fetch real-time web results  
- ğŸ§  Uses **Ollama** with **Llama 3 (8B)** model for local text generation  
- ğŸ› ï¸ Built using **LangChain Runnables + ChatPromptTemplate**  
- ğŸ’¡ RAG pipeline (Search â†’ Prompt â†’ LLM)  
- âš¡ Fast, lightweight, and runs fully on your machine  
- ğŸ–¥ï¸ Simple CLI chat interface  

---
## â¡ï¸ Workflow
![Workflow Diagram](Image2.png)
---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Install Python dependencies

```bash
pip install langchain_community langchain_core duckduckgo-search ollama
```

## 2ï¸âƒ£ Install & Run Ollama

Download Ollama (Windows, Mac, Linux):
ğŸ‘‰ https://ollama.com/download

Pull the required model:

```bash
ollama pull llama3:8b
```


Start the Ollama service:

```bash
ollama serve
```

## â–¶ï¸ Running the Assistant

Run the script:
```bash
python assistant.py
```


You will see:
```bash
ğŸ¤– Hello! I'm a real-time AI assistant. What's new?
You:
```

Ask anything:

```bash
You: Who is the father of computer?
ğŸ¤– Thinking...
ğŸ¤– Based on the search results...
```


To exit:

```bash
You: exit
```



## ğŸ¤– OUPUT 
![Result screenshot](Image.png)


ğŸ“ Troubleshooting
Missing module?
```bash
pip install ddgs
```
